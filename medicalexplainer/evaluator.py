import os
import time
import logging
from pathlib import Path
import plotly.express as px
import plotly.graph_objects as go
from medicalexplainer.dataset import Dataset
from medicalexplainer.llm import models
from medicalexplainer.logger import configure_logger
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama

configure_logger(name="evaluator", filepath=Path(__file__).parent / "logs/medicalexplainer.log")
logger = logging.getLogger("evaluator")


class Evaluator:
    """
    Class for evaluating the LLM
    """
    def evaluate_answer(self, question: str, answer_llm: str, expected_answer: str, context: str) -> str:
        """
        Evaluate the answers obtained from the LLM

        Args:
            question (str): The question to process
            answer_llm (str): The answer generated by the LLM
            expected_answer (str): The expected answer from the dataset
            context (str): The medical context from the dataset

        Returns:
            str: The evaluation result (YES/NO)
        """
        template = """Here is a medical question with its context and the answer that was generated by LLM:
        Context: "{context}"
        Question: "{question}"
        Answer LLM: "{answer_LLM}"
        Compare it with the expected answer:
        "{answer}"
        Is the answer generated by the LLM almost correct compared to the expected answer?
        You ONLY can answer YES/NO"""

        prompt = ChatPromptTemplate.from_template(template)
        llm = models["gemma-3-27b"]

        chain = (
            prompt
            | llm.llm
            | StrOutputParser()
        )

        answer = chain.invoke({
            "context": context,
            "question": question,
            "answer_LLM": answer_llm,
            "answer": expected_answer
        })
        logger.debug(f"Question: {question}, Answer LLM: {answer_llm}, Expected Answer: {expected_answer}, Comparison: {answer}")
        return answer

    def evaluate(self, models_to_evaluate: list, json_data_path: str, tools: bool = False) -> None:
        """
        Evaluates the models on medical questions from JSON dataset.

        Args:
            models_to_evaluate (list): List of models to evaluate.
            json_data_path (str): Path to the JSON dataset file.
            tools (bool): Whether to use tools or not.
        """
        for model in models_to_evaluate:
            all_results = []

            try:
                logger.debug(f"Loading dataset from: {json_data_path}")
                dataset = Dataset(json_data_path)

                logger.debug(f"Processing {len(dataset.dataset_items)} items with model: {model}")

                for idx, item in enumerate(dataset.dataset_items):
                    context = item['context']
                    question = item['question']
                    expected_answer = item['answer']

                    logger.debug(f"Processing question {idx + 1}/{len(dataset.dataset_items)}: {question[:50]}... with model: {model}")

                    for attempt in range(10):
                        logger.debug(f"Attempting to process question {idx + 1}, attempt: {attempt + 1}")
                        try:
                            # Initialize LLM for the current model
                            llm = models[f"{model}"](tools=tools)

                            # Step 1: Generate sub-questions
                            if not isinstance(llm.llm, ChatOllama):
                                time.sleep(2.5)
                            subquestions = llm.get_subquestions(question)
                            logger.debug(f"Generated {len(subquestions)} subquestions for question {idx + 1}")

                            # Step 2: Answer each sub-question with context
                            answers = []
                            for subquestion in subquestions:
                                if not isinstance(llm.llm, ChatOllama):
                                    time.sleep(2.5)
                                answer = llm.answer_subquestion(subquestion, context)
                                answers.append(answer)

                            # Step 3: Get final synthesized answer
                            if not isinstance(llm.llm, ChatOllama):
                                time.sleep(2.5)
                            final_answer = llm.get_final_answer(question, subquestions, answers)

                            # Evaluate the answer
                            try:
                                if not isinstance(llm.llm, ChatOllama):
                                    time.sleep(2)
                                answers_eval = self.evaluate_answer(question, final_answer, expected_answer, context)
                            except Exception as e:
                                logger.error(f"Error evaluating answer: {e}")
                                answers_eval = "PROBLEM"

                            if answers_eval != "PROBLEM":
                                break

                        except Exception as e:
                            logger.error(f"Error processing question {idx + 1}: {e}")
                            answers_eval = "PROBLEM"
                            time.sleep(10)

                    all_results.append({
                        "model": model,
                        "question_id": idx,
                        "question": question,
                        "answer_eval": answers_eval
                    })

                    if tools:
                        logger.info(f"Model: {model}_tools, Question ID: {idx}, Answer Eval: {answers_eval}")
                    else:
                        logger.info(f"Model: {model}, Question ID: {idx}, Answer Eval: {answers_eval}")

            except Exception as e:
                logger.error(f"Error processing dataset with model {model}: {e}")
                time.sleep(10)

            logger.debug("Generating pie charts")
            self.generate_pie_charts(all_results, tools)
            logger.debug("Generating bar charts")
            self.generate_bar_charts(all_results, tools)

    def generate_bar_charts(self, results: list, tools: bool = False) -> None:
        """
        Generate grouped bar charts for correct/incorrect answers per question for each model.
        """
        from collections import defaultdict

        model_question_data = defaultdict(lambda: defaultdict(lambda: {'YES': 0, 'NO': 0}))

        for result in results:
            model = result["model"]
            question_id = result["question_id"]
            eval = result["answer_eval"]

            if eval in ['YES', 'NO']:
                model_question_data[model][question_id][eval] += 1

        for model, questions in model_question_data.items():
            sorted_question_ids = sorted(questions.keys())

            yes_values = []
            no_values = []
            q_labels = []

            for q_id in sorted_question_ids:
                yes_values.append(questions[q_id]['YES'])
                no_values.append(questions[q_id]['NO'])
                q_labels.append(f"Q{q_id}")

            fig = go.Figure(data=[
                go.Bar(name='Correct (YES)', x=q_labels, y=yes_values, marker_color='#4CAF50'),
                go.Bar(name='Incorrect (NO)', x=q_labels, y=no_values, marker_color='#F44336')
            ])

            if tools:
                title = f"Correct and incorrect answers by question: {model} with tools"
            else:
                title = f"Correct and incorrect answers by question: {model}"

            fig.update_layout(
                barmode='group',
                title=title,
                xaxis_title="Questions",
                yaxis_title="Count",
                legend=dict(orientation="h", yanchor="bottom", y=1.02),
                width=1200,
                height=600,
                margin=dict(t=60)
            )

            dir_path = ""
            if tools:
                dir_path = f"medicalexplainer/data/evaluation/{model}_tools/"
            else:
                dir_path = f"medicalexplainer/data/evaluation/{model}/"
            os.makedirs(dir_path, exist_ok=True)

            with open(f"{dir_path}answers.txt", "w") as f:
                f.write(f"Model: {model}\n")
                f.write(f"Correct and incorrect answers:\n")
                for q_id in sorted_question_ids:
                    f.write(f"Question {q_id}: Correct: {questions[q_id]['YES']}, Incorrect: {questions[q_id]['NO']}\n")
                    logger.info(f"Model: {model}, Question {q_id}, Correct: {questions[q_id]['YES']}, Incorrect: {questions[q_id]['NO']}")

            fig.write_image(f"{dir_path}grouped_bar_answers.png")

    def generate_pie_charts(self, results: list, tools: bool = False) -> None:
        """
        Generate pie charts from the evaluation results.

        Args:
            results (list): List of evaluation results.
        """
        from collections import defaultdict, OrderedDict

        model_data = defaultdict(list)
        for result in results:
            model_data[result["model"]].append(result["answer_eval"])

        for model, evaluations in model_data.items():
            counts = OrderedDict([
                ("Correct (YES)", 0),
                ("Incorrect (NO)", 0),
                ("Problematic (PROBLEM)", 0)
            ])

            for eval in evaluations:
                if eval == "YES":
                    counts["Correct (YES)"] += 1
                elif eval == "NO":
                    counts["Incorrect (NO)"] += 1
                else:
                    counts["Problematic (PROBLEM)"] += 1

            total = sum(counts.values())
            if total == 0:
                continue

            labels = list(counts.keys())
            values = [round((count/total)*100, 2) for count in counts.values()]

            if tools:
                title = f"Correct and incorrect answers: {model} with tools"
            else:
                title = f"Correct and incorrect answers: {model}"

            fig = px.pie(
                names=labels,
                values=values,
                title=title,
                color_discrete_sequence=px.colors.qualitative.Pastel
            )

            dir_path = ""
            if tools:
                dir_path = f"medicalexplainer/data/evaluation/{model}_tools/"
            else:
                dir_path = f"medicalexplainer/data/evaluation/{model}/"
            os.makedirs(dir_path, exist_ok=True)

            with open(f"{dir_path}answers_pie_chart.txt", "w") as f:
                f.write(f"Model: {model}\n")
                f.write(f"Correct and incorrect answers:\n")
                for label, value in zip(labels, values):
                    f.write(f"{label}: {value}%\n")
                    logger.info(f"Model: {model}, {label}: {value}%")

            fig.write_image(f"{dir_path}answers_pie_chart.png")
